{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Model Selection via Regularization\n",
    "In this exercise, we will introduce regularization terms in our regression models to prevent overfitting. We will compare the effects of L2 (Ridge) to that of L1 (LASSO) regularization on the values of coefficients. For this exercise, we will be using the cars dataset. It is provided as <b>cars.csv</b> in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import utils\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Read the data as a pandas dataframe. For refernece: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "filename = 'cars.csv'\n",
    "df = utils.read_dataset_from_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(0a). Print the number of observations in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(0b). Print all the columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(0c).Produce a Scatter plot of all variables against each other. <br> \n",
    "Feel free to use the <b>scatter_plot_dataframe()</b> function in utils.py. <br>\n",
    "Note: this function call may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(0d). Produce a plot of correaltions between all variables. <br>\n",
    "Feel free to use the <b>correlation_plot()</b> function in utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(0e). Using the plots above, which variables have a (roughly) linear relationship with 'mpg'? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Type your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<b> (1). Ridge Regression.</b> <br>\n",
    "We will run Ridge regression by introducing an L2 penalty on the regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(1a). Build a simple OLS model using <b>statsmodels.OLS</b> <br>\n",
    "Your dependent variable is 'mpg'. <br>\n",
    "The independent variables are 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb'. <br>\n",
    "Do include the  intercept using the add_constant() function in statsmodels. <br>\n",
    "Hint: http://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html <br>\n",
    "Store your multi-variate model in a variable called <b>sv_model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(1b). Use statsmodels' <b>fit_regularized()</b> function to write a function <b>get_sv_ridge()</b> which fits the model with an L2 penalty of weight $\\alpha$ on an OLS model. Your function should take as arguments the statsmodels OLS model and $\\alpha$ and return the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(1c). Use <b>get_sv_ridge()</b> to plot the Ridge regression coefficients vs. $\\alpha$ for each independent variable and for vs. $log(\\alpha)$ (use log base 10) for each independent variable and for $\\alpha$ in the range [0.1,100.1] in small increments.<br>\n",
    "Ensure that your legend clearly labels all lines (one per each variable) and the axes are appropriately labeled (x-axis should have $log(\\alpha)$ and the y-axis should have the parameter values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(1d). What do you notice about the coefficients as the regularization penalty in increased? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Write your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<b>(2). LASSO (Least Absolute Shrinkage and Selection Operator). </b> <br>\n",
    "We will now run LASSO by introducing an L1 penalty on the regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(2a). Use statsmodels' <b>fit_regularized()</b> function to write a function <b>get_sv_lasso()</b> which fits the model with an L1 penalty of weight $\\alpha$ and returns the output. Your function should take as arguments the statsmodels OLS model and $\\alpha$ and return the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(2b). Use <b>get_sv_lasso()</b> to plot the LASSO coefficients vs. $log(\\alpha)$ (use log base 10) for each independent variable and for $\\alpha$ in the range [0.1,100.1] in small increments. <br>\n",
    "Ensure that your legend clearly labels all lines (one per each variable) and the axes are appropriately labeled (x-axis should have $log(\\alpha)$ and the y-axis should have the parameter values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "(2c). What do you notice about the coefficients as the regularization penalty is increased? Contrast the behavior with what you observed for Ridge Regression? Which might you prefer for model selection, i.e. choosing a sparse subset of features (columns) in a regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Write your answer here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
